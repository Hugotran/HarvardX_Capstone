---
title: "Validation"
author: "Trần Phú Hòa"
date: "March 15, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Content
movieId + userId + date_of_rating + genres + movie_year

# Remove and free up working space

```{r Remove and free up working space, echo = FALSE}
rm(list = ls(all.names = TRUE))
gc()
memory.limit()
memory.size()
```

# Load useful packages
```{r Load useful packages, echo = FALSE}

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")

#to create training, testing and validation splits
if(!require(rsample)) install.packages("rsample", repos = "http://cran.us.r-project.org") 

if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
#
#if(!require(broom)) install.packages("broom", repos = "http://cran.us.r-project.org")

# To find RMSE
if(!require(Metrics)) install.packages("Metrics", repos = "http://cran.us.r-project.org")

#Random forest model
if(!require(ranger)) install.packages("ranger", repos = "http://cran.us.r-project.org")
```


```{r}
memory.size()
```

# Load exd1 data
```{r}
load("tidy_data.RData")
memory.size()
```

# Remove data
```{r}
rm("tidy_edx", "edx1")

gc()
memory.size()

```

## Measuring the test performance
```{r Best model in testing_set}
rm(list = ls(all.names = TRUE))
gc()
memory.limit()
memory.size()

library(tidyverse)
library(ranger)
library(rsample)
library(tictoc)
library(Metrics)
library(caret)
library(Rborist)

tic()

load("tidy_edx.RData")
load("tidy_validation.RData")

set.seed(1234)

sample <- sample_n(tidy_edx, 0.1 * nrow(tidy_edx))

model_ranging <- ranger(formula = rating ~.,
                        data = sample,
                        mtry = 4, num.trees = 100, seed = 42)


actual_rating <- tidy_validation$rating

predict_rating <- predict(model_ranging, tidy_validation)$predictions

rmse(actual_rating, predict_rating)

toc()
# 1.001416
# 3885.44 sec elapsed
```


```{r}
RMSE <- function(test_actual, test_predict){
    sqrt(mean((test_actual - test_predict)^2))
}
RMSE(test_actual, test_predict)
```

#############################
############################
# TESTING MY CODE IN SMALL DATA SET

```{r}
#Create training set and testing set
set.seed(2)
gap_split <- initial_split(mnist_27$train, prop = 0.75)
training_set <- training(gap_split)
dim(training_set)
testing_set <- testing(gap_split)
dim(testing_set)

#Create validation set
cv_split <- vfold_cv(training_set, v = 3) #v: how many times the data should split

#Mapping train & validate
cv_data <- cv_split %>%
  mutate(train = map(splits, ~training(.x)),
         validate = map(splits, ~testing(.x)))
```

```{r Logistic Regression}
# Logistic regression models
cv_models_lr <- cv_data %>%
  mutate(model = map(train, ~ glm(formula = y ~.,
                                 data = .x, family = "binomial")))
#Prepare for cross validated performance
cv_prep_lr <- cv_models_lr %>% 
  mutate(
    # Prepare binary vector of actual Attrition values in validate
    validate_actual = map(validate, ~.x$y == "2"),
    # Prepare binary vector of predicted Attrition values for validate
    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") > 0.5)
  )

# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_lr %>% 
  mutate(validate_recall = map2_dbl(validate_actual, validate_predicted, 
                                    ~recall(actual = .x, predicted = .y)))

# Print the validate_recall column
cv_perf_recall$validate_recall

# Calculate the average of the validate_recall column
mean(cv_perf_recall$validate_recall)
```

### Random forest model

```{r TUne the Hyper-Parameters}
#Tune the Hyper-Parameters
cv_tune <- cv_data %>% crossing(mtry = 1:2)
```

```{r Random forest model}
cv_models_tunerf <- cv_tune %>%
  mutate(model = map2(train, mtry, ~ ranger(formula = y ~.,
                                    data = .x, mtry = .y, seed = 42)))
# Generate validate predictions for each model
cv_prep_tunerf <- cv_models_tunerf %>% 
  mutate(validate_actual = map(validate, ~.x$rating),
         validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y)$predictions))
         
# Calculate validate RMSE for each fold and mtry combination
cv_eval_tunerf <- cv_prep_tunerf %>% 
  mutate(validate_rmse = map2_dbl(.x = validate_actual, .y = validate_predicted, ~accuracy(actual = .x, predicted = .y)))
         
# Calculate the mean validate_mae for each mtry used  
cv_eval_tunerf %>% 
  group_by(mtry) %>% 
  summarise(mean_rmse = mean(validate_rmse))
```

## Measuring the test performance
```{r Best model in testing_set}
best_model <- ranger(formula = rating ~.,
                     data = edx1,
                     mtry = 4, num.trees = 100, seed = 42)
test_actual <- testing_data$life_expectancy
test_predict <- predict(best_model, testing_data)$predictions

rmse(test_actual, test_predict)
```

* Way to create training set and testing set, I way to test how efficient they are in running
```{r First way to create training set and testing set using "caret" package, echo = FALSE}
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
```

```{r Second way to create training set and testing set using "rsample" package, echo = FALSE}
#Create training set and testing set
set.seed(1)
gap_split <- initial_split(tidy_edx, prop = 0.75)
training_set <- training(gap_split)
dim(training_set)
testing_set <- testing(gap_split)
dim(testing_set)

```

Example to plot the correlation plots
mtCor <- cor(mtcars)
library(corrplot)
corrplot(mtCor, method = "ellipse")

* Displaying decision tree models
library(rpart)
treeModel <- rpart(mpg ~ ., data = mtcars)
plot(treeModel)
text(treeModel, cex = 1.6, col = "red", xpd = TRUE)

